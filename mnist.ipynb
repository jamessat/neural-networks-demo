{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhZ1vdjcGTOHx9ZDzOxPRY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamessat/neural-networks-demo/blob/main/mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "654590ba",
        "outputId": "26fa9de4-7c2a-47c6-d893-f28afba983f6"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "# Load the MNIST dataset using Keras\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Flatten the images\n",
        "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# Normalize the pixel values\n",
        "x_train_norm = x_train_flat.astype('float32') / 255.0\n",
        "x_test_norm = x_test_flat.astype('float32') / 255.0\n",
        "\n",
        "# We will keep the labels as integers for now, one-hot encoding will be handled later\n",
        "y_train_int = y_train\n",
        "y_test_int = y_test\n",
        "\n",
        "\n",
        "print(\"Original shapes:\")\n",
        "print(\"x_train:\", x_train.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"x_test:\", x_test.shape)\n",
        "print(\"y_test:\", y_test.shape)\n",
        "\n",
        "print(\"\\nProcessed shapes:\")\n",
        "print(\"x_train_norm:\", x_train_norm.shape)\n",
        "print(\"y_train_int:\", y_train_int.shape)\n",
        "print(\"x_test_norm:\", x_test_norm.shape)\n",
        "print(\"y_test_int:\", y_test_int.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shapes:\n",
            "x_train: (60000, 28, 28)\n",
            "y_train: (60000,)\n",
            "x_test: (10000, 28, 28)\n",
            "y_test: (10000,)\n",
            "\n",
            "Processed shapes:\n",
            "x_train_norm: (60000, 784)\n",
            "y_train_int: (60000,)\n",
            "x_test_norm: (10000, 784)\n",
            "y_test_int: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7d80e22",
        "outputId": "b8161605-197c-4503-823c-c1b984d69c8e"
      },
      "source": [
        "# Define the number of input, hidden, and output units\n",
        "input_size = x_train_norm.shape[1] # 28 * 28 = 784\n",
        "hidden_size = 128 # This is a hyperparameter you can tune\n",
        "output_size = 10 # 10 digits (0-9)\n",
        "\n",
        "# Initialize weights and biases\n",
        "# Weights are initialized with small random values (using a normal distribution)\n",
        "# Biases are initialized with zeros\n",
        "\n",
        "# Weights for input to hidden layer\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "# Biases for hidden layer\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "\n",
        "# Weights for hidden to output layer\n",
        "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "# Biases for output layer\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "print(\"Shapes of initialized parameters:\")\n",
        "print(\"W1:\", W1.shape)\n",
        "print(\"b1:\", b1.shape)\n",
        "print(\"W2:\", W2.shape)\n",
        "print(\"b2:\", b2.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of initialized parameters:\n",
            "W1: (784, 128)\n",
            "b1: (1, 128)\n",
            "W2: (128, 10)\n",
            "b2: (1, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78c33f04"
      },
      "source": [
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the ReLU activation function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- Output of relu(Z), same shape as Z\n",
        "    \"\"\"\n",
        "    A = np.maximum(0, Z)\n",
        "    return A\n",
        "\n",
        "def softmax(Z):\n",
        "    \"\"\"\n",
        "    Implement the Softmax activation function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- numpy array of shape (1, number of output units) or (number of examples, number of output units)\n",
        "\n",
        "    Returns:\n",
        "    A -- Output of softmax(Z), same shape as Z\n",
        "    \"\"\"\n",
        "    # Subtract the maximum value for numerical stability\n",
        "    exp_scores = np.exp(Z - np.max(Z, axis=-1, keepdims=True))\n",
        "    A = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
        "    return A\n",
        "\n",
        "# We'll also need the derivatives for backpropagation\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single ReLU unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    Z -- input of the activation function (used to compute the gradient)\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    dZ = np.array(dA, copy=True) # just converting dA to a numpy array\n",
        "    # When Z <= 0, set dZ to 0\n",
        "    dZ[Z <= 0] = 0\n",
        "    return dZ\n",
        "\n",
        "# Note: The derivative of the softmax function combined with the cross-entropy loss\n",
        "# is simply (A - Y), where A is the output probabilities and Y is the true labels (one-hot encoded).\n",
        "# We will handle this directly in the backward propagation step for the output layer."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "145ec805"
      },
      "source": [
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the two-layer neural network.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data (number of examples, input size)\n",
        "    W1 -- weights for the hidden layer (input size, hidden size)\n",
        "    b1 -- biases for the hidden layer (1, hidden size)\n",
        "    W2 -- weights for the output layer (hidden size, output size)\n",
        "    b2 -- biases for the output layer (1, output size)\n",
        "\n",
        "    Returns:\n",
        "    A2 -- the output of the softmax activation, also known as the post-activation value of the second layer\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" needed for the backward pass\n",
        "    \"\"\"\n",
        "    # Layer 1: Linear -> ReLU\n",
        "    Z1 = X @ W1 + b1\n",
        "    A1 = relu(Z1)\n",
        "\n",
        "    # Layer 2: Linear -> Softmax\n",
        "    Z2 = A1 @ W2 + b2\n",
        "    A2 = softmax(Z2)\n",
        "\n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "\n",
        "    return A2, cache"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8b7512e",
        "outputId": "a43aee3b-81f5-4dc9-e2d6-81bddadad305"
      },
      "source": [
        "def one_hot_encode(Y, num_classes):\n",
        "    \"\"\"\n",
        "    Converts a vector of labels into a one-hot encoded matrix.\n",
        "\n",
        "    Arguments:\n",
        "    Y -- numpy array of labels (shape: (number of examples,))\n",
        "    num_classes -- the total number of unique classes (e.g., 10 for MNIST digits)\n",
        "\n",
        "    Returns:\n",
        "    one_hot_Y -- one-hot encoded matrix (shape: (number of examples, num_classes))\n",
        "    \"\"\"\n",
        "    one_hot_Y = np.zeros((Y.size, num_classes))\n",
        "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "    return one_hot_Y\n",
        "\n",
        "def compute_loss(A2, Y):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy loss.\n",
        "\n",
        "    Arguments:\n",
        "    A2 -- The output of the softmax activation (shape: (number of examples, num_classes))\n",
        "    Y -- The true labels, one-hot encoded (shape: (number of examples, num_classes))\n",
        "\n",
        "    Returns:\n",
        "    loss -- The cross-entropy loss\n",
        "    \"\"\"\n",
        "    m = Y.shape[0] # Number of examples\n",
        "    # Avoid log(0) by clipping probabilities\n",
        "    epsilon = 1e-12\n",
        "    A2 = np.clip(A2, epsilon, 1. - epsilon)\n",
        "    loss = -np.sum(Y * np.log(A2)) / m\n",
        "    return loss\n",
        "\n",
        "# Convert training and test labels to one-hot encoding\n",
        "num_classes = output_size # Already defined as 10\n",
        "y_train_one_hot = one_hot_encode(y_train_int, num_classes)\n",
        "y_test_one_hot = one_hot_encode(y_test_int, num_classes)\n",
        "\n",
        "print(\"Shape of one-hot encoded training labels:\", y_train_one_hot.shape)\n",
        "print(\"Shape of one-hot encoded test labels:\", y_test_one_hot.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of one-hot encoded training labels: (60000, 10)\n",
            "Shape of one-hot encoded test labels: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f836a5e1"
      },
      "source": [
        "def backward_propagation(X, Y, cache, W2):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the two-layer neural network.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data (number of examples, input size)\n",
        "    Y -- true labels, one-hot encoded (number of examples, output size)\n",
        "    cache -- dictionary containing \"Z1\", \"A1\", \"Z2\", \"A2\" from the forward pass\n",
        "    W2 -- weights of the output layer (hidden size, output size)\n",
        "\n",
        "    Returns:\n",
        "    grads -- a dictionary with the gradients with respect to W1, b1, W2, and b2\n",
        "    \"\"\"\n",
        "    m = X.shape[0] # Number of examples\n",
        "\n",
        "    A1 = cache[\"A1\"]\n",
        "    A2 = cache[\"A2\"]\n",
        "    Z1 = cache[\"Z1\"]\n",
        "    Z2 = cache[\"Z2\"]\n",
        "\n",
        "    # Backward pass for the output layer (Linear -> Softmax)\n",
        "    # The derivative of the loss with respect to Z2 is A2 - Y for cross-entropy loss with Softmax\n",
        "    dZ2 = A2 - Y\n",
        "\n",
        "    # Gradients for W2 and b2\n",
        "    dW2 = (A1.T @ dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Backward pass for the hidden layer (Linear -> ReLU)\n",
        "    # Gradient of loss with respect to A1\n",
        "    dA1 = dZ2 @ W2.T\n",
        "\n",
        "    # Gradient of loss with respect to Z1 (using ReLU backward)\n",
        "    dZ1 = relu_backward(dA1, Z1)\n",
        "\n",
        "    # Gradients for W1 and b1\n",
        "    dW1 = (X.T @ dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d14fc038"
      },
      "source": [
        "def update_parameters(W1, b1, W2, b2, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Updates parameters using the gradient descent update rule.\n",
        "\n",
        "    Arguments:\n",
        "    W1 -- weights matrix of shape (input_size, hidden_size)\n",
        "    b1 -- bias vector of shape (1, hidden_size)\n",
        "    W2 -- weights matrix of shape (hidden_size, output_size)\n",
        "    b2 -- bias vector of shape (1, output_size)\n",
        "    grads -- python dictionary containing gradients with respect to weights and biases\n",
        "    learning_rate -- the learning rate for the update rule\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing the updated parameters\n",
        "    \"\"\"\n",
        "    # Retrieve gradients from grads dictionary\n",
        "    dW1 = grads[\"dW1\"]\n",
        "    db1 = grads[\"db1\"]\n",
        "    dW2 = grads[\"dW2\"]\n",
        "    db2 = grads[\"db2\"]\n",
        "\n",
        "    # Update rule for each parameter\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f5667e9",
        "outputId": "d1377365-2931-4fdf-e5a5-30cf74895c46"
      },
      "source": [
        "def train_model(X_train, Y_train, input_size, hidden_size, output_size, num_epochs, learning_rate):\n",
        "    \"\"\"\n",
        "    Trains the two-layer neural network using gradient descent.\n",
        "\n",
        "    Arguments:\n",
        "    X_train -- training data (number of training examples, input size)\n",
        "    Y_train -- true training labels, one-hot encoded (number of training examples, output size)\n",
        "    input_size -- size of the input layer\n",
        "    hidden_size -- size of the hidden layer\n",
        "    output_size -- size of the output layer\n",
        "    num_epochs -- number of iterations over the training set\n",
        "    learning_rate -- the learning rate for parameter updates\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing the learned parameters (W1, b1, W2, b2)\n",
        "    losses -- list of loss values recorded during training\n",
        "    \"\"\"\n",
        "    # Initialize parameters\n",
        "    W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "    b1 = np.zeros((1, hidden_size))\n",
        "    W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "    b2 = np.zeros((1, output_size))\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(num_epochs):\n",
        "\n",
        "        # Forward propagation\n",
        "        A2, cache = forward_propagation(X_train, W1, b1, W2, b2)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = compute_loss(A2, Y_train)\n",
        "        losses.append(loss)\n",
        "\n",
        "        # Backward propagation\n",
        "        grads = backward_propagation(X_train, Y_train, cache, W2)\n",
        "\n",
        "        # Update parameters\n",
        "        parameters = update_parameters(W1, b1, W2, b2, grads, learning_rate)\n",
        "\n",
        "        W1 = parameters[\"W1\"]\n",
        "        b1 = parameters[\"b1\"]\n",
        "        W2 = parameters[\"W2\"]\n",
        "        b2 = parameters[\"b2\"]\n",
        "\n",
        "\n",
        "        # Print loss every 100 epochs\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Loss after epoch {i}: {loss}\")\n",
        "\n",
        "    return parameters, losses\n",
        "\n",
        "# Define hyperparameters for training\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Train the model\n",
        "# We use the normalized training data and one-hot encoded training labels\n",
        "parameters, training_losses = train_model(x_train_norm, y_train_one_hot, input_size, hidden_size, output_size, num_epochs, learning_rate)\n",
        "\n",
        "print(\"\\nTraining finished.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0: 2.3017757126696137\n",
            "Loss after epoch 100: 0.9279257529774099\n",
            "Loss after epoch 200: 0.5009679703239036\n",
            "Loss after epoch 300: 0.40353735491703335\n",
            "Loss after epoch 400: 0.3604952243782641\n",
            "Loss after epoch 500: 0.33505287314039967\n",
            "Loss after epoch 600: 0.31695553546949523\n",
            "Loss after epoch 700: 0.3025398892415277\n",
            "Loss after epoch 800: 0.29021999934160875\n",
            "Loss after epoch 900: 0.27928379058359437\n",
            "\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d7e5b29"
      },
      "source": [
        "def predict(X, parameters):\n",
        "    \"\"\"\n",
        "    Makes predictions using the learned parameters.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data (number of examples, input size)\n",
        "    parameters -- python dictionary containing the learned parameters (W1, b1, W2, b2)\n",
        "\n",
        "    Returns:\n",
        "    predictions -- numpy array of predicted classes (shape: (number of examples,))\n",
        "    \"\"\"\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "    # Forward propagation to get the output probabilities\n",
        "    A2, cache = forward_propagation(X, W1, b1, W2, b2)\n",
        "\n",
        "    # Get the predicted class by finding the index of the maximum probability\n",
        "    predictions = np.argmax(A2, axis=1)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def evaluate_accuracy(predictions, Y_true):\n",
        "    \"\"\"\n",
        "    Calculates the accuracy of the predictions.\n",
        "\n",
        "    Arguments:\n",
        "    predictions -- numpy array of predicted classes (shape: (number of examples,))\n",
        "    Y_true -- numpy array of true labels (shape: (number of examples,))\n",
        "\n",
        "    Returns:\n",
        "    accuracy -- the accuracy as a float between 0 and 1\n",
        "    \"\"\"\n",
        "    # Ensure Y_true is a 1D array of integer labels\n",
        "    if Y_true.ndim > 1:\n",
        "      Y_true = np.argmax(Y_true, axis=1) # Convert one-hot to integer labels if needed\n",
        "\n",
        "    accuracy = np.mean(predictions == Y_true)\n",
        "    return accuracy\n",
        "\n",
        "# Note: This cell will not be executable until the training in cell 0f5667e9 is complete\n",
        "# and the 'parameters' variable is populated with the trained weights.\n",
        "\n",
        "# # Example usage (will run after training):\n",
        "# test_predictions = predict(x_test_norm, parameters)\n",
        "# test_accuracy = evaluate_accuracy(test_predictions, y_test_int)\n",
        "# print(f\"Accuracy on the test set: {test_accuracy * 100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce34e9e6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Note: This cell will not be fully executable until the training in cell 0f5667e9 is complete\n",
        "# and the 'training_losses' list is populated.\n",
        "\n",
        "# Plot the training loss\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(training_losses)\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.title(\"Training Loss over Epochs\")\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}